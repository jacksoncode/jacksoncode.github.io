# 管道与重定向

## 1 标准输入输出基础

### 1.1 文件描述符

在Linux系统中，每个进程都有三个默认的文件描述符：

- **文件描述符0**：标准输入（stdin）
- **文件描述符1**：标准输出（stdout）
- **文件描述符2**：标准错误（stderr）

```bash
#!/bin/bash
# 文件描述符示例

echo "标准输出到屏幕"
echo "标准错误输出到屏幕" >&2

# 查看当前shell的文件描述符
ls -la /proc/$$/fd/

# 输出示例：
# lrwx------ 1 user user 64 Dec 10 10:00 0 -> /dev/pts/0
# lrwx------ 1 user user 64 Dec 10 10:00 1 -> /dev/pts/0
# lrwx------ 1 user user 64 Dec 10 10:00 2 -> /dev/pts/0
```

### 1.2 基本重定向

#### 1.2.1 输出重定向

```bash
#!/bin/bash
# 输出重定向示例

# 标准输出重定向到文件
echo "Hello, World!" > output.txt

# 追加到文件
echo "追加内容" >> output.txt

# 标准错误重定向
echo "错误信息" 2> error.log

# 同时重定向标准输出和标准错误
echo "输出" > combined.log 2>&1
# 或者使用简写形式
echo "输出" &> combined.log

# 清空文件
> empty_file.txt

# 查看重定向效果
cat output.txt
cat error.log
cat combined.log
```

#### 1.2.2 输入重定向

```bash
#!/bin/bash
# 输入重定向示例

# 从文件读取输入
wc -l < /etc/passwd

# 使用here document
cat << EOF > config.txt
# 配置文件
server_name=example.com
port=8080
max_connections=100
EOF

# 使用here string
grep "root" <<< "root:x:0:0:root:/root:/bin/bash"

# 从字符串读取
tr 'a-z' 'A-Z' <<< "hello world"
```

## 2 高级重定向技术

### 2.1 文件描述符操作

#### 2.1.1 自定义文件描述符

```bash
#!/bin/bash
# 自定义文件描述符示例

# 打开文件用于读写
exec 3<> myfile.txt

# 写入文件
echo "第一行" >&3
echo "第二行" >&3

# 从文件读取
exec 3< myfile.txt
while IFS= read -r line <&3; do
    echo "读取: $line"
done

# 关闭文件描述符
exec 3>&-

# 使用多个文件描述符
exec 4> output1.txt
exec 5> output2.txt

echo "写入到文件1" >&4
echo "写入到文件2" >&5

# 关闭所有自定义文件描述符
exec 4>&-
exec 5>&-
```

#### 2.1.2 文件描述符复制

```bash
#!/bin/bash
# 文件描述符复制示例

# 复制stdout到文件描述符6
exec 6>&1

# 复制stderr到文件描述符7
exec 7>&2

# 将所有输出重定向到文件
echo "这将被重定向到文件" > output.txt 2>&1

# 恢复stdout和stderr
echo "这仍然输出到文件" >&6
echo "这仍然输出到错误" >&7

# 关闭复制的文件描述符
exec 6>&-
exec 7>&-
```

### 2.2 进程替换

#### 2.2.1 基本进程替换

```bash
#!/bin/bash
# 进程替换示例

# 使用进程替换比较两个文件
diff <(ls /bin) <(ls /usr/bin)

# 使用进程替换处理命令输出
wc -l <(ps aux)

# 使用进程替换进行复杂操作
sort <(grep "error" /var/log/syslog) <(grep "error" /var/log/auth.log)

# 使用进程替换创建临时文件
while IFS= read -r line; do
    echo "处理: $line"
done < <(find /tmp -type f -name "*.tmp")
```

#### 2.2.2 高级进程替换

```bash
#!/bin/bash
# 高级进程替换示例

# 并行处理多个文件
paste <(cut -d: -f1 /etc/passwd) <(cut -d: -f6 /etc/passwd) <(cut -d: -f7 /etc/passwd)

# 合并排序后的输出
sort -m <(sort file1.txt) <(sort file2.txt) <(sort file3.txt)

# 实时日志处理
while true; do
    tail -f /var/log/syslog | while IFS= read -r line; do
        if [[ "$line" =~ error ]]; then
            echo "发现错误: $line"
        fi
    done
done

# 使用进程替换进行网络测试
for host in google.com github.com stackoverflow.com; do
    echo "$host: $(ping -c 1 "$host" | grep "time=")"
done
```

## 3 管道技术详解

### 3.1 基本管道使用

#### 3.1.1 简单管道链

```bash
#!/bin/bash
# 基本管道示例

# 统计系统中的用户数量
cat /etc/passwd | wc -l

# 查找大文件
find /var/log -type f -size +1M | xargs ls -lh

# 分析系统进程
ps aux | grep "nginx" | awk '{print $2, $11}'

# 网络连接统计
netstat -tuln | grep LISTEN | wc -l

# 日志分析
grep "Failed password" /var/log/auth.log | awk '{print $11}' | sort | uniq -c | sort -nr
```

#### 3.1.2 复杂管道链

```bash
#!/bin/bash
# 复杂管道链示例

# 系统资源监控
ps aux | 
    awk 'NR>1 {print $1, $3, $4, $11}' |
    sort -k2 -nr |
    head -10 |
    column -t

# 磁盘使用分析
df -h |
    grep -E '^/dev/' |
    awk '{print $1, $5, $6}' |
    sed 's/%//' |
    sort -k2 -nr |
    awk '$2 > 80 {print "警告: " $1 " 使用率 " $2 "%"}'

# Web服务器日志分析
awk '{print $1, $7, $9}' /var/log/nginx/access.log |
    grep -E ' (4[0-9][0-9]|5[0-9][0-9])$' |
    awk '{print $1, $2}' |
    sort | uniq -c | sort -nr |
    head -20
```

### 3.2 管道与重定向结合

#### 3.2.1 tee命令使用

```bash
#!/bin/bash
# tee命令示例

# 同时输出到屏幕和文件
ls -la /etc | tee directory_listing.txt

# 追加到文件
ps aux | tee -a process_list.txt

# 使用多个tee
systemctl status nginx | 
    tee full_status.txt |
    grep "Active:" |
    tee active_status.txt |
    awk '{print $2}'

# 调试管道
some_command | tee debug1.txt | another_command | tee debug2.txt | final_command
```

#### 3.2.2 管道中的错误处理

```bash
#!/bin/bash
# 管道错误处理示例

# 捕获管道错误
set -o pipefail

# 错误处理
if ! some_command | another_command | final_command; then
    echo "管道中的某个命令失败了"
fi

# 分离stdout和stderr
some_command 2> >(while read line; do echo "ERROR: $line"; done) |
    while read line; do echo "OUTPUT: $line"; done

# 使用临时文件处理复杂情况
{
    some_command 2> errors.txt | tee output.txt
    if [ -s errors.txt ]; then
        echo "有错误发生:"
        cat errors.txt
    fi
}
```

## 4 高级管道技巧

### 4.1 并行处理

#### 4.1.1 使用xargs并行

```bash
#!/bin/bash
# xargs并行处理示例

# 并行处理文件
find /var/log -name "*.log" -print0 | 
    xargs -0 -P 4 -I {} gzip {}

# 并行网络测试
for host in google.com github.com stackoverflow.com; do
    echo "$host"
done | xargs -P 3 -I {} sh -c 'echo "Testing {}"; ping -c 1 {}'

# 并行压缩
find /backup -name "*.tar" -print0 |
    xargs -0 -P 2 -I {} tar -czf {}.gz {}
```

#### 4.1.2 使用GNU parallel

```bash
#!/bin/bash
# GNU parallel示例

# 并行处理文件
find /var/log -name "*.log" | 
    parallel gzip {}

# 并行网络测试
parallel ping -c 3 ::: google.com github.com stackoverflow.com

# 并行计算
parallel echo 'scale=1000; 4*a(1)' '|' bc -l ::: 1 2 3 4 5

# 复杂并行任务
find /data -name "*.csv" | 
    parallel --progress --eta 'python process.py {} > {.}.processed'
```

### 4.2 流处理

#### 4.2.1 实时流处理

```bash
#!/bin/bash
# 实时流处理示例

# 实时系统监控
while true; do
    {
        echo "=== $(date) ==="
        echo "CPU: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}')"
        echo "Memory: $(free -h | grep Mem | awk '{print $3 "/" $2}')"
        echo "Disk: $(df -h / | tail -1 | awk '{print $5}')"
        echo
    } | tee -a /var/log/system_monitor.log
    sleep 60
done

# 实时日志处理
tail -f /var/log/nginx/access.log | 
    awk '{print $1, $7, $9}' |
    while read ip url status; do
        if [ "$status" -ge 400 ]; then
            echo "$(date): $ip 访问 $url 返回 $status" >> /var/log/error_alerts.log
        fi
    done
```

#### 4.2.2 复杂数据流

```bash
#!/bin/bash
# 复杂数据流处理示例

# 多阶段数据处理
process_large_dataset() {
    local input_file="$1"
    local output_file="$2"
    
    # 阶段1: 数据清洗
    cat "$input_file" |
        grep -v '^#' |
        sed 's/[[:space:]]\+/ /g' |
        awk 'NF > 3' |
    
    # 阶段2: 数据转换
    awk -F' ' '{
        $1 = toupper($1)
        $2 = sprintf("%.2f", $2)
        $3 = strftime("%Y-%m-%d", $3)
        print $0
    }' |
    
    # 阶段3: 数据聚合
    sort -k1,1 |
    awk '{
        key = $1
        sum[key] += $2
        count[key]++
    } END {
        for (k in sum) {
            print k, sum[k]/count[k], count[k]
        }
    }' |
    
    # 阶段4: 格式化输出
    sort -k2 -nr |
    awk '{
        printf "%-20s %8.2f %6d\n", $1, $2, $3
    }' > "$output_file"
}

# 使用示例
# process_large_dataset input.txt output.txt
```

## 5 实用管道重定向技巧

### 5.1 调试技巧

```bash
#!/bin/bash
# 调试技巧

# 管道调试
set -x  # 开启调试
ls -la /etc | grep "conf" | wc -l
set +x  # 关闭调试

# 逐步调试
step1=$(ls -la /etc)
step2=$(echo "$step1" | grep "conf")
step3=$(echo "$step2" | wc -l)
echo "最终结果: $step3"

# 使用临时文件调试
temp1=$(mktemp)
temp2=$(mktemp)

ls -la /etc > "$temp1"
grep "conf" "$temp1" > "$temp2"
wc -l < "$temp2"

rm -f "$temp1" "$temp2"
```

### 5.2 性能优化

```bash
#!/bin/bash
# 性能优化技巧

# 避免不必要的cat
# 不推荐
cat file.txt | grep "pattern"
# 推荐
grep "pattern" file.txt

# 使用更高效的工具
# 慢
ps aux | grep nginx | awk '{print $2}'
# 快
pgrep nginx

# 批量处理
# 慢
for file in *.log; do
    grep "ERROR" "$file"
done
# 快
grep "ERROR" *.log

# 使用内建命令
# 慢
echo $(date)
# 快
date
```

### 5.3 安全考虑

```bash
#!/bin/bash
# 安全考虑

# 避免命令注入
# 不安全
eval "ls $user_input"
# 安全
ls -- "$user_input"

# 处理文件名中的空格和特殊字符
find . -name "*.txt" -print0 | xargs -0 rm -f

# 验证输入
while IFS= read -r line; do
    if [[ "$line" =~ ^[a-zA-Z0-9._-]+$ ]]; then
        # 安全处理
        echo "处理: $line"
    else
        echo "跳过不安全输入: $line"
    fi
done < input.txt
```

## 6 综合实例

### 6.1 系统监控管道

```bash
#!/bin/bash
# 系统监控管道

# 创建系统监控报告
system_report() {
    local report_file="/tmp/system_report_$(date +%Y%m%d_%H%M%S).txt"
    
    {
        echo "=== 系统监控报告 ==="
        echo "时间: $(date)"
        echo "主机: $(hostname)"
        echo ""
        
        echo "=== 进程统计 ==="
        ps aux |
            awk 'NR>1 {print $1, $3, $4, $11}' |
            sort -k2 -nr |
            head -10 |
            column -t
        
        echo ""
        echo "=== 磁盘使用 ==="
        df -h |
            grep -E '^/dev/' |
            awk '{print $1, $5, $6}' |
            sed 's/%//' |
            sort -k2 -nr |
            awk '$2 > 80 {print "警告: " $1 " 使用率 " $2 "% 挂载点 " $3}'
        
        echo ""
        echo "=== 网络连接 ==="
        ss -tuln |
            grep LISTEN |
            awk '{print $1, $5}' |
            sort -k2 -n
        
        echo ""
        echo "=== 系统负载 ==="
        uptime |
            awk -F'load average:' '{print "负载: " $2}'
    } > "$report_file"
    
    echo "报告已生成: $report_file"
}

# 运行监控
system_report
```

### 6.2 日志分析管道

```bash
#!/bin/bash
# 日志分析管道

# 分析Web服务器日志
analyze_web_logs() {
    local log_file="/var/log/nginx/access.log"
    local report_file="/tmp/web_analysis_$(date +%Y%m%d_%H%M%S).txt"
    
    if [ ! -f "$log_file" ]; then
        echo "日志文件不存在: $log_file"
        return 1
    fi
    
    {
        echo "=== Web服务器日志分析 ==="
        echo "文件: $log_file"
        echo "时间: $(date)"
        echo ""
        
        echo "=== 访问统计 ==="
        awk '{print $1}' "$log_file" |
            sort | uniq -c | sort -nr |
            head -10 |
            awk '{printf "%-15s %6d 次\n", $2, $1}'
        
        echo ""
        echo "=== 状态码统计 ==="
        awk '{print $9}' "$log_file" |
            sort | uniq -c | sort -nr |
            awk '{printf "状态码 %3s: %6d 次\n", $2, $1}'
        
        echo ""
        echo "=== 错误请求 ==="
        awk '$9 >= 400 {print $1, $7, $9}' "$log_file" |
            sort | uniq -c | sort -nr |
            head -10 |
            awk '{printf "%-15s %-30s %3s 次\n", $2, $3, $1}'
        
        echo ""
        echo "=== 请求路径统计 ==="
        awk '{print $7}' "$log_file" |
            grep -v "^$" |
            sort | uniq -c | sort -nr |
            head -20 |
            awk '{printf "%-40s %6d 次\n", $2, $1}'
        
        echo ""
        echo "=== 用户代理统计 ==="
        awk -F'"' '{print $6}' "$log_file" |
            sort | uniq -c | sort -nr |
            head -10 |
            awk '{printf "%-50s %6d 次\n", substr($2,1,50), $1}'
    } > "$report_file"
    
    echo "Web日志分析报告: $report_file"
}

# 运行分析
analyze_web_logs
```

这些示例展示了管道和重定向的强大功能，从基本的输入输出重定向到复杂的流处理和并行计算。掌握这些技术可以显著提高命令行操作的效率和灵活性。