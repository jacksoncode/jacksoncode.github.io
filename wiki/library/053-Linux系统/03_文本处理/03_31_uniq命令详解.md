# 03_31_uniq命令详解

## 1. 命令概述

`uniq`命令是Linux系统中一个用于处理文本文件中重复行的工具。它的主要功能是识别和处理相邻的重复行，可以去除重复行、只显示重复行或统计每行的出现次数。`uniq`命令特别适合于处理已经排序过的文件，常与`sort`命令配合使用，在数据清洗、日志分析、统计报表生成等场景中有着广泛的应用。

- **去除重复行**：删除文件中相邻的重复行，只保留一行
- **显示重复行**：只显示文件中出现重复的行
- **统计重复次数**：显示每行在文件中出现的次数
- **忽略指定字段**：可以在比较时忽略指定的字段或字符
- **自定义比较范围**：可以指定比较的字符范围
- **灵活的输入输出**：支持从标准输入读取数据，输出到标准输出或指定文件

## 2. 语法格式

`uniq`命令的基本语法格式如下：

```bash
uniq [选项]... [输入文件 [输出文件]]
```

其中：
- `[选项]`：控制去重行为和输出格式的参数
- `[输入文件]`：要处理的文件路径，如果不指定或使用`-`，则从标准输入读取数据
- `[输出文件]`：输出结果的文件路径，如果不指定，则输出到标准输出

## 3. 常用选项

| 选项 | 说明 | 示例 |
|------|------|------|
| `-c` 或 `--count` | 显示每行在文件中出现的次数 | `uniq -c file.txt` |
| `-d` 或 `--repeated` | 只显示出现重复的行（至少出现两次的行） | `uniq -d file.txt` |
| `-D` 或 `--all-repeated` | 显示所有重复的行，包括每行的所有重复实例 | `uniq -D file.txt` |
| `-f N` 或 `--skip-fields=N` | 在比较时忽略前N个字段 | `uniq -f 1 file.txt` |
| `-i` 或 `--ignore-case` | 比较时忽略大小写差异 | `uniq -i file.txt` |
| `-s N` 或 `--skip-chars=N` | 在比较时忽略前N个字符 | `uniq -s 2 file.txt` |
| `-u` 或 `--unique` | 只显示不重复的行（只出现一次的行） | `uniq -u file.txt` |
| `-w N` 或 `--check-chars=N` | 只比较每行的前N个字符 | `uniq -w 3 file.txt` |
| `--help` | 显示帮助信息 | `uniq --help` |
| `--version` | 显示版本信息 | `uniq --version` |

## 4. 基本用法

### 4.1 默认去重

**示例1：去除文件中的重复行**

假设有一个已排序的文件`sorted_file.txt`，内容为：

```
apple
apple
banana
cherry
cherry
cherry
date
```

执行以下命令：

```bash
uniq sorted_file.txt
```

输出结果：

```
apple
banana
cherry
date
```

默认情况下，`uniq`命令会删除相邻的重复行，只保留一行。需要注意的是，`uniq`命令只会处理相邻的重复行，因此通常需要先对文件进行排序。

### 4.2 统计重复次数

**示例2：显示每行的出现次数**

使用示例1中的文件，执行以下命令：

```bash
uniq -c sorted_file.txt
```

输出结果：

```
      2 apple
      1 banana
      3 cherry
      1 date
```

此命令使用`-c`选项，显示每行在文件中出现的次数，次数显示在行号前面，用空格填充。

### 4.3 只显示重复行

**示例3：只显示至少出现两次的行**

使用示例1中的文件，执行以下命令：

```bash
uniq -d sorted_file.txt
```

输出结果：

```
apple
cherry
```

此命令使用`-d`选项，只显示出现重复的行（至少出现两次的行），每个重复的行只显示一次。

### 4.4 只显示不重复行

**示例4：只显示出现一次的行**

使用示例1中的文件，执行以下命令：

```bash
uniq -u sorted_file.txt
```

输出结果：

```
banana
date
```

此命令使用`-u`选项，只显示不重复的行（只出现一次的行）。

### 4.5 输出到文件

**示例5：将结果输出到指定文件**

```bash
uniq sorted_file.txt unique_file.txt
```

此命令将去重后的结果输出到`unique_file.txt`文件中，而不是输出到标准输出。

## 5. 高级用法与技巧

### 5.1 显示所有重复行实例

**示例6：显示所有重复的行（包括每个实例）**

使用示例1中的文件，执行以下命令：

```bash
uniq -D sorted_file.txt
```

输出结果：

```
apple
apple
cherry
cherry
cherry
```

此命令使用`-D`选项，显示所有重复的行，包括每行的所有重复实例。

### 5.2 忽略前N个字段

**示例7：在比较时忽略前N个字段**

假设有一个文件`data_with_ids.txt`，内容为：

```
1 apple
2 apple
3 banana
4 cherry
5 cherry
```

执行以下命令：

```bash
sort -k 2 data_with_ids.txt | uniq -f 1
```

输出结果：

```
1 apple
3 banana
4 cherry
```

此命令先按第二列排序，然后使用`-f 1`选项忽略第一列（ID字段），对其余部分进行去重。

### 5.3 忽略前N个字符

**示例8：在比较时忽略前N个字符**

假设有一个文件`data_with_prefix.txt`，内容为：

```
A_apple
B_apple
C_banana
D_cherry
E_cherry
```

执行以下命令：

```bash
sort -k 1.3 data_with_prefix.txt | uniq -s 2
```

输出结果：

```
A_apple
C_banana
D_cherry
```

此命令先按从第三个字符开始的部分排序，然后使用`-s 2`选项忽略前两个字符（包括下划线），对其余部分进行去重。

### 5.4 只比较前N个字符

**示例9：只比较每行的前N个字符**

假设有一个文件`partial_match.txt`，内容为：

```
apple_pie
apple_jam
banana_split
cherry_pie
cherry_jam
```

执行以下命令：

```bash
sort partial_match.txt | uniq -w 5
```

输出结果：

```
apple_pie
banana_split
cherry_pie
```

此命令先对文件进行排序，然后使用`-w 5`选项只比较每行的前5个字符，因此`apple_pie`和`apple_jam`被视为相同，`cherry_pie`和`cherry_jam`也被视为相同。

### 5.5 忽略大小写

**示例10：比较时忽略大小写差异**

假设有一个文件`mixed_case.txt`，内容为：

```
Apple
apple
Banana
banana
Cherry
```

执行以下命令：

```bash
sort mixed_case.txt | uniq -i
```

输出结果：

```
Apple
Banana
Cherry
```

此命令先对文件进行排序，然后使用`-i`选项在比较时忽略大小写差异。

## 6. 实用技巧

### 6.1 与sort命令结合使用

**示例11：去重未排序的文件**

```bash
sort file.txt | uniq
```

由于`uniq`命令只处理相邻的重复行，因此通常需要先使用`sort`命令对文件进行排序，然后再进行去重。这是`uniq`命令最常见的用法之一。

### 6.2 查找重复的单词或短语

**示例12：在文本中查找重复的单词**

```bash
cat text.txt | tr -s '[:space:]' '\n' | sort | uniq -d
```

此命令组合使用`tr`将文本中的空白字符替换为换行符，将文本转换为单词列表，然后使用`sort`和`uniq -d`查找重复的单词。

### 6.3 统计文件中单词的出现频率

**示例13：统计文本中每个单词的出现次数**

```bash
cat text.txt | tr -s '[:space:]' '\n' | sort | uniq -c | sort -nr
```

此命令组合使用`tr`、`sort`、`uniq -c`和再次`sort -nr`，先将文本转换为单词列表，然后统计每个单词的出现次数，并按出现次数降序排序。

### 6.4 查找重复的文件

**示例14：基于内容查找重复的文件**

```bash
find . -type f -exec md5sum {} \; | sort | uniq -w 32 -d
```

此命令使用`find`和`md5sum`计算每个文件的MD5哈希值，然后使用`sort`和`uniq -w 32 -d`查找具有相同哈希值的文件（即内容相同的文件）。

### 6.5 处理日志文件中的重复条目

**示例15：去除日志文件中的重复条目**

```bash
sort access.log | uniq > unique_access.log
```

此命令对访问日志进行排序并去重，去除重复的日志条目。

### 6.6 查找系统中的重复用户

**示例16：查找/etc/passwd中具有相同用户ID的用户**

```bash
cut -d: -f3,1 /etc/passwd | sort | uniq -f 1 -d
```

此命令使用`cut`提取用户ID和用户名，然后使用`sort`和`uniq -f 1 -d`查找具有相同用户ID的用户。

### 6.7 结合其他命令处理CSV数据

**示例17：处理CSV文件中的重复行**

```bash
# 按特定列去重
cut -d, -f2 data.csv | sort | uniq -c
# 或保留完整行但按特定列去重
sort -t, -k2 data.csv | uniq -f1 -w0
```

第一个命令提取CSV文件的第二列，然后统计每个值的出现次数；第二个命令按第二列排序，并按第二列去重，同时保留完整的行。

### 6.8 生成唯一的文件名列表

**示例18：生成不重复的文件名列表**

```bash
ls -1 | sort | uniq > unique_files.txt
```

此命令使用`ls -1`列出当前目录下的文件（每行一个），然后排序并去重，生成不重复的文件名列表。

## 7. 常见问题与解决方案

### 7.1 去重效果不佳

**问题：** 使用`uniq`命令后，文件中仍然存在重复行
**解决方案：** 确保在使用`uniq`命令前先对文件进行排序

```bash
sort file.txt | uniq  # 正确：先排序再去重
```

### 7.2 无法处理不相邻的重复行

**问题：** 文件中存在不相邻的重复行，但`uniq`命令无法识别
**解决方案：** 先对文件进行排序，使重复行相邻

```bash
sort file.txt | uniq  # 先排序使重复行相邻，再去重
```

### 7.3 比较时需要忽略某些字段

**问题：** 文件中每行包含多个字段，但只需要比较特定字段
**解决方案：** 使用`-f`选项忽略前N个字段

```bash
sort -k 2 file.txt | uniq -f 1  # 忽略第一列，按第二列去重
```

### 7.4 大小写敏感问题

**问题：** `uniq`命令默认区分大小写，导致相同单词的不同大小写形式被视为不同
**解决方案：** 使用`-i`选项忽略大小写差异

```bash
sort file.txt | uniq -i  # 忽略大小写去重
```

### 7.5 输出格式不符合要求

**问题：** `uniq -c`命令的输出中，计数和文本之间有很多空格
**解决方案：** 使用`awk`或`sed`进一步处理输出格式

```bash
sort file.txt | uniq -c | awk '{print $1","$2}'  # 将计数和文本用逗号分隔
```

### 7.6 处理非常大的文件

**问题：** 处理非常大的文件时，`sort | uniq`命令组合可能消耗大量内存和时间
**解决方案：** 分块处理大文件

```bash
split -l 10000 large_file.txt chunk_
for i in chunk_*; do
sort $i | uniq > ${i}_unique
rm $i
done
sort -m chunk_*_unique | uniq > final_unique.txt
rm chunk_*_unique
```

### 7.7 无法正确处理包含特殊字符的行

**问题：** 文件中包含特殊字符（如换行符、制表符等），影响`uniq`命令的处理
**解决方案：** 预处理文件，转义或替换特殊字符

```bash
sed 's/\t/ /g' file.txt | sort | uniq  # 将制表符替换为空格
```

## 8. 相关命令对比

| 命令 | 主要特点 | 适用场景 |
|------|---------|---------|
| `uniq` | 处理相邻的重复行，常与sort结合使用 | 去重操作，统计重复行出现次数
| `sort` | 强大的排序工具，支持多种排序方式 | 文本排序，与uniq结合进行全局去重
| `comm` | 比较两个已排序的文件，找出共同和不同的行 | 比较两个已排序文件的差异和交集
| `diff` | 比较两个文件的差异，无需预先排序 | 比较文件差异，不局限于重复行
| `join` | 基于共同字段连接两个已排序的文件 | 基于关键字段合并数据
| `fdupes` | 查找重复文件，基于文件内容比较 | 查找系统中的重复文件
| `dedupe` | 更高级的去重工具，支持更多选项 | 复杂的去重需求
| `perl`/`python` | 编程语言中的去重实现 | 复杂的文本处理和去重逻辑

## 9. 实践练习

### 9.1 基础练习

1. 使用`uniq`命令对简单文本文件进行去重，观察默认行为
2. 尝试使用`-c`、`-d`、`-u`等选项进行不同类型的去重操作
3. 学习将`uniq`命令与`sort`命令结合使用

### 9.2 中级练习

1. 处理包含多个字段的文件，使用`-f`和`-s`选项忽略特定字段或字符
2. 练习使用`uniq -c`和`sort -nr`统计并排序单词或行的出现频率
3. 尝试使用`-i`选项忽略大小写去重

### 9.3 高级练习

1. 编写一个脚本，使用`uniq`命令和其他工具处理日志文件，统计访问频率
2. 创建一个文件去重工具，使用`find`、`md5sum`和`uniq`命令查找并处理重复文件
3. 对比不同去重方法的性能差异，特别是在处理大文件时

## 10. 总结

`uniq`命令是Linux系统中一个简单而实用的文本处理工具，主要用于识别和处理文件中的相邻重复行。它提供了多种选项来控制去重的行为，包括去除重复行、只显示重复行、统计重复次数等。`uniq`命令特别适合于与`sort`命令结合使用，实现对文件的全局去重操作。

通过`uniq`命令，用户可以轻松地处理文本文件中的重复数据，在数据清洗、日志分析、统计报表生成等场景中发挥重要作用。`uniq`命令的设计简洁而高效，虽然它的功能相对专一，但与其他文本处理命令结合使用时，可以实现更复杂的数据处理任务。

在使用`uniq`命令时，需要注意以下几点：

1. 默认情况下，`uniq`命令只会处理相邻的重复行，因此通常需要先对文件进行排序
2. 使用`-c`、`-d`、`-u`等选项可以实现不同的去重需求
3. `uniq`命令支持忽略特定字段或字符、忽略大小写等高级功能
4. 结合其他文本处理命令（如`sort`、`tr`、`cut`、`awk`等），可以实现更复杂的数据处理任务

总之，`uniq`命令是Linux文本处理工具集中的一个重要成员，它提供了一种简单而有效的方法来处理文本文件中的重复行，帮助用户更好地组织、分析和处理数据。通过实践和熟悉各种选项的使用，用户可以充分发挥`uniq`命令的功能，提高文本处理的效率和质量。